{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x74EizVvSzKL",
        "outputId": "3c28bf77-8d96-4ec7-b13e-f72e83d5c641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Setup complete\n"
          ]
        }
      ],
      "source": [
        "!pip -q install \"gymnasium[atari]\" \"ale-py\" \"tensorboard\"\n",
        "import gymnasium as gym\n",
        "import ale_py\n",
        "gym.register_envs(ale_py)\n",
        "print(\"âœ… Setup complete\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import ale_py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStackObservation\n",
        "import time\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "\n",
        "# AGGRESSIVE HYPERPARAMETERS FOR A100 (TURBO MODE)\n",
        "NUM_ENVS = 32          # Increased for A100 parallelism\n",
        "N_STEPS = 128\n",
        "TOTAL_TIMESTEPS = 5_000_000\n",
        "LEARNING_RATE = 2.5e-4 # Slightly lower LR for stability with larger batch\n",
        "GAMMA = 0.99\n",
        "GAE_LAMBDA = 0.95\n",
        "ENT_COEF = 0.01\n",
        "VALUE_COEF = 0.5\n",
        "CLIP_COEF = 0.1        # Tighter clipping for stability\n",
        "MAX_GRAD_NORM = 0.5\n",
        "NUM_EPOCHS = 4\n",
        "BATCH_SIZE = 1024      # Larger batch for A100 efficiency\n",
        "STOP_REWARD = 19.0"
      ],
      "metadata": {
        "id": "yItO1IXHXYou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        meanings = self.env.unwrapped.get_action_meanings()\n",
        "        self._fire_actions = [i for i, m in enumerate(meanings) if m == \"FIRE\"] or [1]\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        for a in self._fire_actions[:2]:\n",
        "            obs, _, terminated, truncated, _ = self.env.step(a)\n",
        "            if terminated or truncated:\n",
        "                obs, info = self.env.reset(**kwargs)\n",
        "        return obs, info\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        super().__init__(env)\n",
        "        self.noop_max = noop_max\n",
        "        meanings = self.env.unwrapped.get_action_meanings()\n",
        "        self.noop_action = meanings.index(\"NOOP\") if \"NOOP\" in meanings else 0\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        noops = np.random.randint(1, self.noop_max + 1)\n",
        "        for _ in range(noops):\n",
        "            obs, _, terminated, truncated, _ = self.env.step(self.noop_action)\n",
        "            if terminated or truncated:\n",
        "                obs, info =self.env.reset(**kwargs)\n",
        "        return obs, info\n",
        "\n",
        "class RestrictActions(gym.ActionWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        meanings = self.env.unwrapped.get_action_meanings()\n",
        "        allowed = []\n",
        "\n",
        "        # Find NOOP\n",
        "        noop_candidates = [i for i, m in enumerate(meanings) if m == \"NOOP\"]\n",
        "        allowed.append(noop_candidates[0] if noop_candidates else 0)\n",
        "\n",
        "        # Find UP (In Pong, LEFT moves paddle UP)\n",
        "        up_candidates = [i for i, m in enumerate(meanings) if m == \"UP\" or m == \"LEFT\"]\n",
        "        allowed.append(up_candidates[0] if up_candidates else 0)\n",
        "\n",
        "        # Find DOWN (In Pong, RIGHT moves paddle DOWN)\n",
        "        down_candidates = [i for i, m in enumerate(meanings) if m == \"DOWN\" or m == \"RIGHT\"]\n",
        "        allowed.append(down_candidates[0] if down_candidates else 0)\n",
        "\n",
        "        self.allowed_actions = allowed\n",
        "        self.action_space = gym.spaces.Discrete(len(self.allowed_actions))\n",
        "        print(f\"RestrictActions: Mapped {allowed} to (NOOP, UP, DOWN)\")\n",
        "\n",
        "    def action(self, a):\n",
        "        return int(self.allowed_actions[int(a)])\n",
        "\n",
        "def make_pong_env():\n",
        "    def thunk():\n",
        "        env = gym.make(\"ALE/Pong-v5\", frameskip=1, full_action_space=False, repeat_action_probability=0.0)\n",
        "        env = AtariPreprocessing(env, screen_size=84, grayscale_obs=True, frame_skip=4, scale_obs=False)\n",
        "        env = NoopResetEnv(env, noop_max=30)\n",
        "        env = FireResetEnv(env)\n",
        "        env = RestrictActions(env)\n",
        "        env = FrameStackObservation(env, stack_size=4)\n",
        "        return env\n",
        "    return thunk"
      ],
      "metadata": {
        "id": "j7TZXhAsXbzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCriticCNN(nn.Module):\n",
        "    def __init__(self, action_dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, 8, 4), nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, 2), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, 1), nn.ReLU(),\n",
        "        )\n",
        "        self.fc = nn.Sequential(nn.Linear(64 * 7 * 7, 512), nn.ReLU())\n",
        "        self.policy = nn.Linear(512, action_dim)\n",
        "        self.value = nn.Linear(512, 1)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "        nn.init.orthogonal_(self.policy.weight, gain=0.01)\n",
        "        nn.init.orthogonal_(self.value.weight, gain=1.0)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        if obs.dtype != torch.float32:\n",
        "            obs = obs.float()\n",
        "        if obs.max() > 1.0:\n",
        "            obs = obs / 255.0\n",
        "        x = self.conv(obs)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.policy(x), self.value(x)\n",
        "\n",
        "def compute_gae(rewards, values, dones, next_value, gamma, gae_lambda):\n",
        "    advantages = torch.zeros_like(rewards)\n",
        "    last_gae = 0\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        if t == len(rewards) - 1:\n",
        "            next_val = next_value\n",
        "        else:\n",
        "            next_val = values[t + 1]\n",
        "        delta = rewards[t] + gamma * next_val * (1 - dones[t]) - values[t]\n",
        "        advantages[t] = last_gae = delta + gamma * gae_lambda * (1 - dones[t]) * last_gae\n",
        "    returns = advantages + values\n",
        "    return returns, advantages"
      ],
      "metadata": {
        "id": "GCV5qfzkXcwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"PPO for Pong - A100 TURBO MODE\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Envs: {NUM_ENVS} | Steps: {N_STEPS} | Batch: {BATCH_SIZE}\")\n",
        "    print(f\"LR: {LEARNING_RATE} | Target: {STOP_REWARD}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Use AsyncVectorEnv for parallelism\n",
        "    from gymnasium.vector import AsyncVectorEnv\n",
        "    envs = AsyncVectorEnv([make_pong_env() for _ in range(NUM_ENVS)])\n",
        "\n",
        "    model = ActorCriticCNN(3).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
        "    writer = SummaryWriter(comment=\"-ppo_pong\")\n",
        "\n",
        "    obs = torch.zeros((N_STEPS, NUM_ENVS, 4, 84, 84), dtype=torch.uint8, device=device)\n",
        "    actions = torch.zeros((N_STEPS, NUM_ENVS), dtype=torch.long, device=device)\n",
        "    logprobs = torch.zeros((N_STEPS, NUM_ENVS), device=device)\n",
        "    rewards = torch.zeros((N_STEPS, NUM_ENVS), device=device)\n",
        "    dones = torch.zeros((N_STEPS, NUM_ENVS), device=device)\n",
        "    values = torch.zeros((N_STEPS, NUM_ENVS), device=device)\n",
        "\n",
        "    global_step = 0\n",
        "    next_obs = torch.tensor(envs.reset()[0], dtype=torch.uint8, device=device)\n",
        "    next_done = torch.zeros(NUM_ENVS, device=device)\n",
        "    num_updates = TOTAL_TIMESTEPS // (N_STEPS * NUM_ENVS)\n",
        "\n",
        "    # Manual Episode Tracking\n",
        "    running_ep_rewards = np.zeros(NUM_ENVS)\n",
        "    running_ep_lengths = np.zeros(NUM_ENVS)\n",
        "    ep_returns = []\n",
        "    best_reward = float('-inf')\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"\\n{'Update':<8} {'Steps':<12} {'Episodes':<10} {'AvgRew':<10} {'MaxRew':<10} {'FPS':<10} {'Time':<8}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    for update in range(1, num_updates + 1):\n",
        "        for step in range(N_STEPS):\n",
        "            global_step += NUM_ENVS\n",
        "            obs[step] = next_obs\n",
        "            dones[step] = next_done\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits, value = model(next_obs)\n",
        "                probs = torch.distributions.Categorical(logits=logits)\n",
        "                action = probs.sample()\n",
        "                logprob = probs.log_prob(action)\n",
        "\n",
        "            values[step] = value.flatten()\n",
        "            actions[step] = action\n",
        "            logprobs[step] = logprob\n",
        "\n",
        "            next_obs_np, reward, terminated, truncated, infos = envs.step(action.cpu().numpy())\n",
        "            rewards[step] = torch.tensor(reward, device=device)\n",
        "            next_obs = torch.tensor(next_obs_np, dtype=torch.uint8, device=device)\n",
        "            next_done = torch.tensor(np.logical_or(terminated, truncated), dtype=torch.float32, device=device)\n",
        "\n",
        "            # Manual tracking update\n",
        "            running_ep_rewards += reward\n",
        "            running_ep_lengths += 1\n",
        "\n",
        "            for idx, (term, trunc) in enumerate(zip(terminated, truncated)):\n",
        "                if term or trunc:\n",
        "                    ep_r = running_ep_rewards[idx]\n",
        "                    ep_l = running_ep_lengths[idx]\n",
        "                    ep_returns.append(ep_r)\n",
        "                    writer.add_scalar(\"charts/episodic_return\", ep_r, global_step)\n",
        "                    writer.add_scalar(\"charts/episodic_length\", ep_l, global_step)\n",
        "                    running_ep_rewards[idx] = 0\n",
        "                    running_ep_lengths[idx] = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            nextvalue = model(next_obs)[1].flatten()\n",
        "        returns, advantages = compute_gae(rewards, values, dones, nextvalue, GAMMA, GAE_LAMBDA)\n",
        "\n",
        "        b_obs = obs.reshape((-1, 4, 84, 84))\n",
        "        b_logprobs = logprobs.reshape(-1)\n",
        "        b_actions = actions.reshape(-1)\n",
        "        b_advantages = advantages.reshape(-1)\n",
        "        b_returns = returns.reshape(-1)\n",
        "        b_values = values.reshape(-1)\n",
        "\n",
        "        b_inds = np.arange(N_STEPS * NUM_ENVS)\n",
        "        for epoch in range(NUM_EPOCHS):\n",
        "            np.random.shuffle(b_inds)\n",
        "            for start in range(0, N_STEPS * NUM_ENVS, BATCH_SIZE):\n",
        "                end = start + BATCH_SIZE\n",
        "                mb_inds = b_inds[start:end]\n",
        "\n",
        "                mb_obs = b_obs[mb_inds]\n",
        "                newlogits, newvalue = model(mb_obs)\n",
        "                newprobs = torch.distributions.Categorical(logits=newlogits)\n",
        "                newlogprob = newprobs.log_prob(b_actions[mb_inds])\n",
        "                entropy = newprobs.entropy().mean()\n",
        "\n",
        "                logratio = newlogprob - b_logprobs[mb_inds]\n",
        "                ratio = logratio.exp()\n",
        "                mb_advantages = b_advantages[mb_inds]\n",
        "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "                pg_loss1 = -mb_advantages * ratio\n",
        "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - CLIP_COEF, 1 + CLIP_COEF)\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                v_loss = F.mse_loss(newvalue.flatten(), b_returns[mb_inds])\n",
        "                loss = pg_loss + VALUE_COEF * v_loss - ENT_COEF * entropy\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "                optimizer.step()\n",
        "\n",
        "            if epoch == NUM_EPOCHS - 1 and start == 0:\n",
        "                writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
        "                writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
        "                writer.add_scalar(\"losses/entropy\", entropy.item(), global_step)\n",
        "                writer.add_scalar(\"losses/total_loss\", loss.item(), global_step)\n",
        "\n",
        "        if len(ep_returns) > 0:\n",
        "            recent = ep_returns[-100:]\n",
        "            avg_r = np.mean(recent)\n",
        "            max_r = np.max(recent)\n",
        "            best_reward = max(best_reward, max_r)\n",
        "            writer.add_scalar(\"charts/avg_episode_return\", avg_r, global_step)\n",
        "\n",
        "            if update % 5 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                fps = global_step / elapsed\n",
        "                print(f\"{update:<8} {global_step:<12,} {len(ep_returns):<10} {avg_r:<10.2f} {max_r:<10.2f} {fps:<10.0f} {int(elapsed)}s\")\n",
        "\n",
        "            if len(recent) >= 10 and np.mean(recent[-10:]) >= STOP_REWARD:\n",
        "                print(f\"\\nðŸŽ‰ Target {STOP_REWARD} reached! Avg: {np.mean(recent[-10:]):.2f}\")\n",
        "                break\n",
        "\n",
        "    envs.close()\n",
        "    writer.close()\n",
        "    torch.save({'model': model.state_dict(), 'steps': global_step, 'best': best_reward}, \"ppo_pong.pt\")\n",
        "    print(f\"\\n{'='*70}\\nSteps: {global_step:,} | Episodes: {len(ep_returns)} | Best: {best_reward:.2f}\\n{'='*70}\")\n",
        "    print(\"âœ… Saved: ppo_pong.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-B24nEBXgKe",
        "outputId": "b53f880b-4f17-4358-f839-4fc86109fe16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "PPO for Pong - A100 TURBO MODE\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Envs: 32 | Steps: 128 | Batch: 1024\n",
            "LR: 0.00025 | Target: 19.0\n",
            "======================================================================\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "RestrictActions: Mapped [0, 3, 2] to (NOOP, UP, DOWN)\n",
            "\n",
            "Update   Steps        Episodes   AvgRew     MaxRew     FPS        Time    \n",
            "----------------------------------------------------------------------\n",
            "10       40,960       31         -20.65     -19.00     2807       14s\n",
            "15       61,440       49         -20.73     -19.00     2796       21s\n",
            "20       81,920       68         -20.75     -19.00     2808       29s\n",
            "25       102,400      94         -20.81     -19.00     2798       36s\n",
            "30       122,880      117        -20.84     -19.00     2783       44s\n",
            "35       143,360      141        -20.91     -19.00     2756       52s\n",
            "40       163,840      162        -20.90     -20.00     2735       59s\n",
            "45       184,320      184        -20.88     -20.00     2726       67s\n",
            "50       204,800      207        -20.81     -19.00     2715       75s\n",
            "55       225,280      228        -20.74     -18.00     2716       82s\n",
            "60       245,760      246        -20.71     -18.00     2710       90s\n",
            "65       266,240      267        -20.65     -18.00     2706       98s\n",
            "70       286,720      292        -20.58     -18.00     2710       105s\n",
            "75       307,200      311        -20.62     -18.00     2713       113s\n",
            "80       327,680      330        -20.64     -18.00     2718       120s\n",
            "85       348,160      355        -20.63     -18.00     2714       128s\n",
            "90       368,640      370        -20.66     -18.00     2710       136s\n",
            "95       389,120      393        -20.52     -18.00     2708       143s\n",
            "100      409,600      416        -20.31     -17.00     2704       151s\n",
            "105      430,080      428        -20.25     -17.00     2708       158s\n",
            "110      450,560      454        -20.15     -17.00     2700       166s\n",
            "115      471,040      477        -20.17     -17.00     2697       174s\n",
            "120      491,520      494        -20.27     -17.00     2700       182s\n",
            "125      512,000      509        -20.34     -17.00     2701       189s\n",
            "130      532,480      532        -20.19     -16.00     2699       197s\n",
            "135      552,960      550        -20.20     -16.00     2698       204s\n",
            "140      573,440      570        -20.07     -16.00     2697       212s\n",
            "145      593,920      587        -20.00     -16.00     2700       219s\n",
            "150      614,400      607        -19.84     -16.00     2701       227s\n",
            "155      634,880      624        -19.91     -18.00     2705       234s\n",
            "160      655,360      643        -19.80     -16.00     2706       242s\n",
            "165      675,840      657        -19.78     -16.00     2708       249s\n",
            "170      696,320      674        -19.73     -16.00     2709       257s\n",
            "175      716,800      687        -19.67     -16.00     2710       264s\n",
            "180      737,280      700        -19.54     -15.00     2714       271s\n",
            "185      757,760      712        -19.39     -15.00     2715       279s\n",
            "190      778,240      725        -19.22     -13.00     2717       286s\n",
            "195      798,720      736        -19.17     -13.00     2720       293s\n",
            "200      819,200      749        -19.16     -13.00     2721       301s\n",
            "205      839,680      761        -19.06     -13.00     2724       308s\n",
            "210      860,160      767        -19.00     -13.00     2725       315s\n",
            "215      880,640      779        -18.91     -13.00     2728       322s\n",
            "220      901,120      788        -18.82     -13.00     2729       330s\n",
            "225      921,600      797        -18.81     -13.00     2732       337s\n",
            "230      942,080      809        -18.79     -13.00     2733       344s\n",
            "235      962,560      812        -18.81     -13.00     2736       351s\n",
            "240      983,040      822        -18.85     -14.00     2738       359s\n",
            "245      1,003,520    829        -18.78     -14.00     2740       366s\n",
            "250      1,024,000    839        -18.53     -14.00     2741       373s\n",
            "255      1,044,480    846        -18.33     -13.00     2743       380s\n",
            "260      1,064,960    849        -18.32     -13.00     2745       387s\n",
            "265      1,085,440    858        -18.18     -13.00     2747       395s\n",
            "270      1,105,920    862        -18.11     -13.00     2748       402s\n",
            "275      1,126,400    868        -17.82     -5.00      2750       409s\n",
            "280      1,146,880    873        -17.61     -5.00      2751       416s\n",
            "285      1,167,360    879        -17.25     -5.00      2754       423s\n",
            "290      1,187,840    883        -17.01     -5.00      2755       431s\n",
            "295      1,208,320    889        -16.59     -5.00      2756       438s\n",
            "300      1,228,800    894        -16.20     -5.00      2758       445s\n",
            "305      1,249,280    897        -15.86     -5.00      2759       452s\n",
            "310      1,269,760    901        -15.52     -5.00      2761       459s\n",
            "315      1,290,240    908        -14.87     4.00       2761       467s\n",
            "320      1,310,720    911        -14.44     6.00       2763       474s\n",
            "325      1,331,200    916        -13.26     14.00      2763       481s\n",
            "330      1,351,680    922        -12.04     16.00      2765       488s\n",
            "335      1,372,160    927        -10.85     16.00      2765       496s\n",
            "340      1,392,640    936        -8.28      20.00      2765       503s\n",
            "345      1,413,120    942        -6.44      21.00      2766       510s\n",
            "350      1,433,600    950        -4.03      21.00      2766       518s\n",
            "355      1,454,080    955        -2.65      21.00      2767       525s\n",
            "360      1,474,560    963        -0.02      21.00      2768       532s\n",
            "365      1,495,040    971        2.39       21.00      2768       540s\n",
            "370      1,515,520    977        3.95       21.00      2769       547s\n",
            "375      1,536,000    980        4.44       21.00      2769       554s\n",
            "380      1,556,480    991        7.04       21.00      2770       561s\n",
            "385      1,576,960    997        8.66       21.00      2770       569s\n",
            "390      1,597,440    1003       9.74       21.00      2771       576s\n",
            "395      1,617,920    1011       11.60      21.00      2772       583s\n",
            "400      1,638,400    1020       12.68      21.00      2772       591s\n",
            "405      1,658,880    1026       13.43      21.00      2772       598s\n",
            "410      1,679,360    1035       12.68      21.00      2773       605s\n",
            "415      1,699,840    1040       12.77      21.00      2774       612s\n",
            "420      1,720,320    1047       12.81      21.00      2774       620s\n",
            "425      1,740,800    1057       13.29      21.00      2774       627s\n",
            "430      1,761,280    1065       13.13      21.00      2774       634s\n",
            "435      1,781,760    1069       12.90      21.00      2776       641s\n",
            "440      1,802,240    1076       13.13      21.00      2776       649s\n",
            "445      1,822,720    1085       13.82      21.00      2776       656s\n",
            "450      1,843,200    1094       14.21      21.00      2776       663s\n",
            "\n",
            "ðŸŽ‰ Target 19.0 reached! Avg: 19.10\n",
            "\n",
            "======================================================================\n",
            "Steps: 1,851,392 | Episodes: 1098 | Best: 21.00\n",
            "======================================================================\n",
            "âœ… Saved: ppo_pong.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_iframe(6006)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "PYNkmx7FevN_",
        "outputId": "5e3928dc-1192-4652-aa0a-4079a04f27a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(6006, \"/\", \"100%\", \"400\", false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}